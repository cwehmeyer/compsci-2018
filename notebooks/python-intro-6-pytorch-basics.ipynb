{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python VI: pytorch basics\n",
    "\n",
    "## Content\n",
    "- tensors vs arrays\n",
    "- automatic differentiation\n",
    "\n",
    "## Prequisites\n",
    "Visit [pytorch.org](http://pytorch.org) and follow the installation instructions.\n",
    "\n",
    "## Remember jupyter notebooks\n",
    "- To run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>.\n",
    "- To get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.Tensor` vs `numpy.ndarray`\n",
    "We shall see in the next few cells how to create `pytorch`'s main data structure: tensors. We will also see that the syntax is really close to that of `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.ones(3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.zeros(4, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.arange(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the standard `Tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor()\n",
    "print(a)\n",
    "print(a.dim())\n",
    "print(a.shape)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be initialised with (nested) lists..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([0, 1, 2])\n",
    "print(a)\n",
    "print(a.dim())\n",
    "print(a.shape)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[0, 1, 2], [3, 4, 5]])\n",
    "print(a)\n",
    "print(a.dim())\n",
    "print(a.shape)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or with `numpy.ndarray` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor(np.asarray([[0, 1, 2], [3, 4, 5]]))\n",
    "print(a)\n",
    "print(a.dim())\n",
    "print(a.shape)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard `Tensor` defaults to single precision (32 bit), independent of the initial data.\n",
    "\n",
    "The `tensor()` function, however, uses the same type as the supplied data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(np.array([[0, 1, 2], [3, 4, 5]]))\n",
    "print(a)\n",
    "print(a.dim())\n",
    "print(a.shape)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(np.array([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0]]))\n",
    "print(a)\n",
    "print(a.dim())\n",
    "print(a.shape)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exists a special function to create a `Tensor` from a `numpy.ndarray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.from_numpy(np.array([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0]], dtype=np.float32))\n",
    "print(a)\n",
    "print(a.dim())\n",
    "print(a.shape)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, like in `numpy`, you can change a `Tensor`'s type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.from_numpy(np.array([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0]])).float()\n",
    "print(a)\n",
    "print(a.dim())\n",
    "print(a.shape)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `torch.Tensor` is actually a wrapper around a `numpy.ndarray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(6).reshape(-1, 3)\n",
    "b = torch.from_numpy(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:, 1] *= -1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A type cast, however, disconnects array and tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(6).reshape(-1, 3)\n",
    "b = torch.from_numpy(a.astype(np.float))\n",
    "c = torch.from_numpy(a).float()\n",
    "\n",
    "a[:, 1] = -1\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `torch.Tensor`s (nearly) like `numpy.ndarray`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(6).float()\n",
    "\n",
    "print(a + 1)\n",
    "print(a - 1)\n",
    "print(a * 2)\n",
    "print(a / 2)\n",
    "print(a // 2)\n",
    "print(a % 2)\n",
    "print(a**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, unlike arrays, tensors do not change their `dtype`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(a / 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pytorch` is **really** strict about using the right data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = torch.Tensor([[1, 2, 3], [4, 5, 6]]).double()\n",
    "\n",
    "try:\n",
    "    print(a + b)\n",
    "except Exception as e:\n",
    "    print(type(e))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operating on `numpy.ndarray`s usually creates new objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((3, 5))\n",
    "b = np.exp(a)\n",
    "print(id(a))\n",
    "print(id(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make operations **inplace**, we have to make some (small) effort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.exp(a, out=a)\n",
    "print(id(a))\n",
    "print(id(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id(a))\n",
    "a[:] = np.exp(a)\n",
    "print(id(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `pytorch`, the situation is similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3, 5)\n",
    "b = torch.exp(a)\n",
    "print(id(a))\n",
    "print(id(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.exp(a, out=a)\n",
    "print(id(a))\n",
    "print(id(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id(a))\n",
    "a[:] = torch.exp(a)\n",
    "print(id(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are, however, (non-)inplace operations available as methods for `torch.Tensor`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id(a))\n",
    "print(id(a.exp_()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id(a))\n",
    "print(id(a.exp()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a GPU available, using the `cuda()` method moves a `torch.Tensor` onto the GPU and all subsequent calculations are performed there. With the `cpu()` method, we can get our `torch.Tensor` back from the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1000, 1000)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('We have CUDA!')\n",
    "    a = a.cuda()\n",
    "else:\n",
    "    print('No CUDA :(')\n",
    "\n",
    "a.exp_().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens, though, if you call `.cuda()` on a `torch.Tensor` without having a CUDA-compatible GPU at your disposal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    a.cuda()\n",
    "except Exception as e:\n",
    "    print(type(e))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the earlier programming exercises  `mean(a)`, `scalar_product(a, b)`, and `linear_regression(x, y)`?\n",
    "\n",
    "Here, we refactor them for `torch.Tensor`s. We only use methods of already existing `torch.Tensor`s as well as the operators `=`, `-`, `*`, and `/`. For `linear_regression(x, y)` we then use `mean(a)` and `scalar_product(a, b)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(a):\n",
    "    return a.sum().div(len(a))\n",
    "\n",
    "def scalar_product(a, b):\n",
    "    return a.mul(b).sum()\n",
    "\n",
    "def linear_regression(x, y):\n",
    "    x_mean = mean(x)\n",
    "    y_mean = mean(y)\n",
    "    x = x.sub(x_mean)\n",
    "    y = y.sub(y_mean)\n",
    "    slope = scalar_product(x, y) / x.pow(2).sum()\n",
    "    const = y_mean - slope * x_mean\n",
    "    return slope, const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert -0.1 < mean(torch.randn(1000)) < 0.1\n",
    "assert scalar_product(torch.Tensor([0, 1, 2]), torch.Tensor([1, 1, 1])) == 3\n",
    "\n",
    "x = torch.Tensor([10, 14, 16, 15, 16, 20])\n",
    "y = torch.Tensor([ 1,  3,  5,  6,  5, 11])\n",
    "slope, const = linear_regression(x, y)\n",
    "assert 0.97 < slope < 0.99\n",
    "assert -9.72 < const < -9.70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make `pytorch` track which operations are performed on a `torch.Tensor` by using the `requires_grad=True` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3)\n",
    "b = torch.rand_like(a, requires_grad=True)\n",
    "c = torch.sum(a * b**2)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pytorch` is now able to differentiate `c` with respect to `b`.\n",
    "\n",
    "Let's make use of that to solve an actual optimisation problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf(x, y):\n",
    "    \"\"\"Rosenbrock function\"\"\"\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-1, 3, 100))\n",
    "zz = rbf(xx, yy)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.contour(xx, yy, zz, np.linspace(51, 2000, 20), colors='k', linewidths=0.1)\n",
    "ax.contourf(xx, yy, zz, np.linspace(0, 50, 20))\n",
    "ax.plot([-2, 2], [1, 1], '--', linewidth=1, color='C1')\n",
    "ax.plot([1, 1], [-1, 3], '--', linewidth=1, color='C1')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a starting point and require a gradient for this tensor. Then, we evaluate the Rosenbrock function for this position and obtain the gradient via differentiation of the function at this position. And then, we follow the negative gradient and repeat until we converge to the global minimum.\n",
    "\n",
    "In short, we locate the minimum of the Rosenbrock function via steepest descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = torch.tensor([-0.3, 2.8], requires_grad=True)\n",
    "\n",
    "path, conv = [], []\n",
    "while True:\n",
    "    f = rbf(*xy)\n",
    "    path.append(xy.data.numpy().copy())\n",
    "    conv.append(f.item())\n",
    "    if conv[-1] < 0.00001:\n",
    "        break\n",
    "    f.backward()\n",
    "    xy.data.sub_(xy.grad.data.mul_(0.0005))\n",
    "    xy.grad.zero_()\n",
    "path = np.asarray(path)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].plot(conv)\n",
    "axes[0].semilogx()\n",
    "axes[0].semilogy()\n",
    "axes[0].set_xlabel('steps')\n",
    "axes[0].set_ylabel('function value')\n",
    "axes[1].contour(xx, yy, zz, np.linspace(51, 2000, 20), colors='k', linewidths=0.1)\n",
    "axes[1].contourf(xx, yy, zz, np.linspace(0, 50, 20))\n",
    "axes[1].plot(*path.T, linewidth=3, color='C3')\n",
    "axes[1].plot([-2, 2], [1, 1], '--', linewidth=1, color='C1')\n",
    "axes[1].plot([1, 1], [-1, 3], '--', linewidth=1, color='C1')\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].set_xlabel('$x$')\n",
    "axes[1].set_ylabel('$y$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for ax, cut in zip(axes.flat, [2, 4, 11, 101, 1001, 2501, 5001, 10001]):\n",
    "    ax.contour(xx, yy, zz, np.linspace(51, 2000, 20), colors='k', linewidths=0.1)\n",
    "    ax.contourf(xx, yy, zz, np.linspace(0, 50, 20))\n",
    "    ax.plot(*path[:cut].T, '-o', markersize=3, linewidth=1, color='C3')\n",
    "    ax.plot([-2, 2], [1, 1], '--', linewidth=1, color='C1')\n",
    "    ax.plot([1, 1], [-1, 3], '--', linewidth=1, color='C1')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.text(-1.9, -0.8, f'steps: {cut - 1}', fontsize=15)\n",
    "    ax.set_axis_off()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
